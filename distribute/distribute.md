

- [distribute](#distribute)
    - [一致性](#%E4%B8%80%E8%87%B4%E6%80%A7)
    - [数据分布与负载均衡](#%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1)
        - [哈希分布](#%E5%93%88%E5%B8%8C%E5%88%86%E5%B8%83)
        - [顺序分布](#%E9%A1%BA%E5%BA%8F%E5%88%86%E5%B8%83)
        - [数据迁移](#%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB)
    - [复制](#%E5%A4%8D%E5%88%B6)
        - [强同步复制](#%E5%BC%BA%E5%90%8C%E6%AD%A5%E5%A4%8D%E5%88%B6)
        - [异步复制](#%E5%BC%82%E6%AD%A5%E5%A4%8D%E5%88%B6)
    - [分布式协议](#%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE)
        - [两阶段提交协议](#%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE)
        - [Paxos协议](#paxos%E5%8D%8F%E8%AE%AE)
        - [Quorum NRW](#quorum-nrw)

# distribute
## 一致性

-   强一致性：假如A先写入一个值到存储系统，存储系统能够保证后续ABC读取操作都将返回最新值。
-   弱一致性：假如A先写入一个值到存储系统，系统不能保证ABC的读取操作能否读取到最新值。
-   最终一致性：~，后续没有写操作更新同样的值，ABC的读取操作最终都会读取到A写入的最新值。从A写入，到ABC读取到最新值的这段时间，称为“不一致性窗口”。

## 数据分布与负载均衡

负载的衡量有很多因素，load值，cpu，内存，磁盘，网络等。

### 哈希分布

一致哈希的便利在于，集群数量变化时可以方便的扩容和数据迁移（只影响到相邻结点）。另一个特点是，相同id请求总是发到同一台服务器，这样同一个用户id的数据不会被散列到不同服务器，但缺点也是在用户数据量大的时候可能造成过载（还有种说法是所有数据限定在同一个存储节点，无法发挥分布式多机并行处理的能力）。对于大用户，可以将它的数据再进行拆分处理。

数据迁移过程中的负载问题，5.1节

### 顺序分布

哈希分布破坏了数据的有序性，只能支持随机读取，不能支持顺序扫描。  

顺序分布就是将大表划分为连续的小表，与B+树结构类似  

### 数据迁移

负载均衡做？不是吧！

假设数据分片D有两个副本D1D2分别在结点A1A2，D1为主副本，D2为从副本，迁移D1到其他结点的过程为：
1\. 将数据分片D的读写服务有A1切换到A2，D2变成主副本。
2\. 选择一个新的结点B，**从A2结点获取D2的数据**并与之保持同步。
3\. 从A1删除D1副本。

## 复制
复制协议分为两种，强同步和异步复制。**两者的区别在于，是否需要同步到备副本才可以返回成功**。当系统出现故障时，分布式系统需要将服务自动切换到备副本，实现自动容错。  
强同步复制能保证一致性，但是当`备副本`出现故障时，会阻塞存储系统的正常写服务，系统的整体可用性受到影响。  
异步复制可用性好，但是当`主副本`出现故障时会出现数据丢失的可能性。  

### 强同步复制
客户端将**写请求**发送给主副本，主副本将写请求通过同步commit log来复制到其他副本，备副本根据日志进行操作，完成后通知主副本。然后主副本修改本机，等所有操作都完成后通知客户端些完成。  

备副本的个数可能大于1个，实现强同步协议时，主复本可以将操作日志并发地发给所有副本并等待回复，只要有一个备副本返回成功就可以继续操作并恢复客户端操作成功。  

主复本出现故障时，至少有一个备副本可用。

### 异步复制
异步模式下，主副本不需要等待备副本的回应，本地修改成功后就可以通知客户端操作成功。

## 分布式协议
* 分布式事务
一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。

### 两阶段提交协议

二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

所谓的两个阶段是指：第一阶段：`准备阶段(投票阶段)`和第二阶段：`提交阶段（执行阶段）`

提交请求阶段(或者叫做投票阶段)
    1.事务协调者(事务管理器)给每个参与者(cohorts, 资源管理器)发送Prepare消息，等待直到收到所有cohorts的回复。
    2.cohorts在本地节点执行事务(之后协调者会要求提交这个事务)，写本地的redo和undo日志。
    3.每一个cohorts，如果执行成功，回复一个agreement消息(假如cohorts同意执行commit)；如果执行失败，回复一个abort消息。(两阶段提交协议)。

提交阶段(或者叫完成阶段)
成功
如果协调者接收到所有参与者发送回来的agreement消息：
    1.协调者发送一个commit消息给所有的cohorts
    2.每一个参与者完成commit操作，(两阶段提交协议)释放所有事务处理过程中使用的锁资源
    3.每一个参与者回复一个acknowledgment给协调者
    4.协调者在收到所有acknowledgment消息之后完成整个操作
失败
如果任何一个参与者在提交请求阶段回复abort消息给协调者:
    1.协调者回复一个rollback消息给所有的cohorts
    2.每一个参与者执行本地事务的undo操作(根据undo日志记录)，并且释放事务执行过程中使用的资源和锁
    3.每一个参与者给协调者回复acknowledgement消息(两阶段提交协议)。
    4.协调者在接收到所有的参与者的acknowledgement消息之后执行事务undo操作


两阶段提交协议最大的缺点是：它是一个阻塞协议。当一个节点在等待回复消息时进入阻塞状态。其他需要这些资源的处理事务需要等待。如果协调者挂掉，cohorts将永远不能结束它们的事务

### 三阶段提交协议
与两阶段提交不同的是，三阶段提交有两个改动点。

1、引入超时机制。**同时在协调者和参与者中都引入超时机制。**  
2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

### Paxos协议
Paxos协议用于在主节点发生故障时，选取新的结点。  

在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。

一般的NoSQL都会通过数据复制的形式保证其可用性，但客户端对多数据进行操作时，可能会有很多对同一数据的操作发送的某一台或几台Server，有可能执行：Insert、Update A、Update B....Update N，就一次Insert连续多次Update，最终复制Server上也必须执行这一的更新操作，如果因为线程池、网络、Server资源等原因导致各复制Server接收到的更新顺序不一致，这样的复制数据就失去了意义，如果在金融领域甚至会造成严重的后果。

http://blog.csdn.net/dellme99/article/details/14162159


### Quorum NRW
N表示数据所具有的副本数。
R表示完成读操作所需要读取的最小副本数，即一次读操作所需参与的最小节点数目。
W表示完成写操作所需要写入的最小副本数，即一次写操作所需要参与的最小节点数目。
该策略中，只需要保证R + W>N，就可以保证强一致性。 如果R + W ≤ N，这时读取和写入操作是不重叠的，系统只能保证最终一致性，而副本达到一致的时间则依赖于系统异步更新的实现方式，不一致性的时间段也就等于从更新开始到所有的节点都异步完成更新之间的时间。


假设N=5， 如果R=1， 那么W必须是5. 所以就是写入所有的节点是全部节点，那么读取任何一个节点就可以最新的数据。 有点就是像读写锁了。
如果R=5， 那么W只要是1就可以了。 那么写的效率就非常高。 读取的效率比较低。 
如果R=N/2+1， W=N/2， 读写之间为达到某个平衡。 是不错的策略。兼顾了性能和可用性，Dynamo系统的默认设置就是这种。